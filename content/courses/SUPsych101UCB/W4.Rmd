---
title: "Linear Regression"
date: "Week 4"
output:
  rmdformats::readthedown:
    highlight: kate
css: custom.css
---

<style type="text/css">

body{ /* Normal  */
      font-size: 26px;
      line-height: 1.2;
  }
p{ /* Normal  */
      font-size: 26px;
      line-height: 1.2;
  }
 li{ /* Normal  */
      font-size: 24px;
      line-height: 1.2;
       padding: 5px 0px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;

}
h1 { /* Header 1 */
  font-size: 48px;

}
h2 { /* Header 2 */
    font-size: 36px;

}
h3 { /* Header 3 */
  font-size: 30px;


}

h4 { /* Header 4 */
  font-size: 28px;


}

code.r{ /* Code block */
    font-size: 40px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 18px;
}
</style>

```{r setup, echo=FALSE, cache=FALSE, warning = F, message = F}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

```{r, echo = F,cache=FALSE, warning = F, message = F}
library(knitr)
library(formatR)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)

```

## Aims

For continous data:  

Understand linear regression with one predictor and two predictors  

Understand how we assess the fit of a regression model

* Total sum of squares
* Model sum of squares
* Residual sum of squares
* F
* R^2^

Know how to do regression using R  

Interpret a regression model

## What is Regression?

A way of predicting the value of one variable from another

* It is a hypothetical model of the relationship between two variables
* The model used is a linear one
* Therefore, we describe the relationship using the equation of a straight line  

This week focuses on linear regression with **continous predictors (IVs)**. This means our predictors (IVs) will be on a numeric scale of some sort. (Next week we will work on categorical data where data is not numeric, but in categories instead).  

## Describing a Straight Line
![](images/week4/image1.png)


**b~0~:** Represents the **Intercept** (value of Y when X = 0)

* Point at which the regression line crosses the Y-axis (ordinate)


**b~i~:** Regression coefficient for the predictor

* **Slope** of the regression line
  + Direction/strength of relationship
  

## Intercepts and Slopes

![](images/week4/image2.png)

The image on the **left** shows a case where the two lines share the same intercept (value when = 0) but have different slopes (direction & strength of relationship)  

The image on the **right** shows a case where the two lines have different intercepts (value when = 0) but have the same slope (direction & strength of relationship)  

## The Method of Least Squares

![](images/week4/image3.png)

This graph shows a scatterplot of some data with a line representing the general trend  

The vertical lines (dotted) represent the differences (or **residuals**) between the line and the actual data  

## How Good is the Model?

The regression line is only a model based on the data  

This model might not reflect reality

* We need some way of testing how well the model fits the observed data.
* How?

## Sums of Squares

![](images/week4/image4.png)

Diagram showing from where the regression sums of squares derive  

### Sum of Squares Summary

SS~T~: Total variability (variability between scores and the mean)  

SS~R~: Residual/error variability (variability between the regression model and the actual data)  

SS~M~: Model variability (difference in variability between the model and the mean)  

## Testing the Model: ANOVA

![](images/week4/image5.png)


If the model results in better prediction than using the mean, then we expect SS~M~ to be much greater than SS~R~


### Mean Squared Error

Sums of squares are total values  

They can be expressed as averages  

These are called mean squares, MS 

![](images/week4/image6.png)

## Testing the Model: R^2^

**R^2^:** The proportion of variance accounted for by the regression model, aka the Pearson Correlation Coefficient Squared  

![](images/week4/image7.png)

## Regression: An Example

A record company boss was interested in predicting record sales from advertising  

Data

* 200 different album releases

Outcome (Dependent) variable:

* Sales (CDs and downloads) in the week after release

Predictor (Independent) variable:

* The amount (in units of $1000) spent promoting the record before release

### Regression in R

We run a regression analysis using the *lm()* function – lm stands for ‘linear model’  

This function takes the general form:

```{r, eval = F, echo = T}
newModel<-lm(outcome ~ predictor(s), data = dataFrame)
```

*Does amount spent on advertising predict album sales?*  

Applying that formula to our example: 
```{r, eval = F, echo = T}
albumSales.1 <- lm(sales ~ adverts, data = album1)
```


We have created an object called albumSales.1 that contains the results of our analysis  

We can show the object by executing:
```{r, eval = F, echo = T}
summary(albumSales.1)
```

![](images/week4/image8.png)

## Interpreting Simple Regression Output

1. First we will interpret the intercept (*b~0~*). The intercept is the value of our dependent variable when our independent variable is equal to 0. So, when 0$ is spent on advertising, album sales will be 134.1 CD sales/downloads (scientific notation). The p-value is not important for the intercept.  

2. Next, we will interpret the regression coefficient (slope, *b~i~*), our beta, for advertising $ spent in 1k units. The value is 961.2. This means that for every 1 unit increase in advertising (1k), the dependent variable (sales) will increase by 961.2. The p-value is below 0.05 so we can say this is a statistically significant relationship. We can say that spending more mone on advertising predicts higher album sales.  

This is called a **main effect**. We can say that **there was a significant main effect of advertising such that higher advertising spending predicted higher album sales.**  

3. Now we can look at model fit. The R^2^ is aprox .33, this means that advertising $ accounts for roughly 33% of the variation in album sales. This is a good fit!

## Making Predictions with the Regression Formula

![](images/week4/image1.png)

Lets say that the album executives wanted to *predict* how different album sales would be if they spent 2k, 10k, or 100k. Now that we have an intercept and a slope, we can make predictions using our data! That is what the **X~i~** stands for in our formula.

**Spending 2k:**
```{r, include = T, eval = T, echo = T}

#intercept + (slope * X)

134.1 + (961.2 * 2)
```

Spending 2k on adverts would lead to approx 2057 sales.

**Spending 10k:**
```{r, include = T, eval = T, echo = T}

#intercept + (slope * X)

134.1 + (961.2 * 10)
```

Spending 10k on adverts would lead to approx 9746 sales.

**Spending 100k:**
```{r, include = T, eval = T, echo = T}

#intercept + (slope * X)

134.1 + (961.2 * 100)
```

Spending 10k on adverts would lead to approx 96,254 sales.

## Multiple Regression: Main Effects Only

What we have just gone through is called a *simple regression*, meaning we have one predictor (IV) and one outcome (DV) variable. More often than not, what we are interested in is not just one predictor, but the impact of multiple predictors (IVs) on an outcome (DV). This is called **Multiple Regression**. Interpreting the output of a multiple regression is similar to the simple regression with a few tweaks.  

We will be looking at and interpreting main effects. A **main effect** (also called a simple effect) is the effect of one independent variable (predictor) on the dependent variable (outcome). *It ignores the effects of any other independent variables* (Krantz, 2019).  

We will learn about multiple regression and how to interpret the findings through expanding our previous example. A record company boss was interested in predicting record sales from advertising. With simple regression, they found that amount spent on advertising was a significant predictor of higher album sales. Now the boss wants to know if how attractive the band is (on a 1-10 scale) impacts record sales.  

Data

* 200 different album releases

Outcome (Dependent) variable:

* Sales (CDs and downloads) in the week after release

Predictors (Independent) variable:

* The amount (in units of $1000) spent promoting the record before release
* The level of attractiveness (on a 1-10 scale) of the band  

The r syntax follows the same formula:

```{r, eval = F, echo = T}
newModel<-lm(outcome ~ predictor1 + predictor2, data = dataFrame)
```

*Note: You can add more preditors (IVs) by putting additional '+' and variable names* 

```{r}
albumdata<- read.csv("AlbumSales2.csv")
names(albumdata)[1] <- "adverts"
```

The syntax for our multiple regression is:

```{r, echo = T, eval = T}
album.mod <- lm(sales ~ adverts + attract, data = albumdata)
```

Next we must run a summary on our saved model object to get our model output: 
```{r, eval = T, echo = T}
summary(album.mod)
```

#### Interpretation

The **intercept** is the value of our DV (album sales) when *everything* (ALL our IVs) is equal to zero. So, when advertising money AND band attractiveness is 0, it is predicted that there will be 26.34 album sales.  


Next we will look at the **main effect** of advertising. Similar to our simple model, we see that it is still a significant (p < 0.05) predictor of album sales. The b~1~ (1 because we have multiple IVs now) is equal to 0.09. As advertising spending increases by one unit (1k), album sales will increase by 0.09. Because it is statistically significant we can say that: **There is a significant main effect of advertising spending such that increased advertising spending predicts higher album sales.** This main effect is *independent* of band attractiveness, and holds it constant (controls for it) at 0. This means that this main effect is the unique effect of advertising on album sales, independent of band attractiveness.    

Now we have a second predictor (IV) to look at. We will look at the **main effect** of band attractiveness on album sales. Again, this main effect is *independent* of any other predictors (IVs) and is showing us the unique effect of band attractiveness on album sales. The b~2~ (because it is our second predictor) is equal to 16.27. As band attractiveness incrrases by one unit, album sales will increase by 16.27. Because it is statistically significant (p< 0.05) we can say that: **There is a significant main effect of band attractiveness such that higher band attractiveness predicts higher album sales.**  

As far as fit goes, our R^2^ is 0.41. This means that our two predictors (IVs) account for/explain 41% of the variability in album sales. This is a lot! 

### Making Predictions

Lets say that the album executives wanted to *predict* how different album sales would be if they spent more on advertising or focused on band attractiveness. Now that we have an intercept and our slopes, we can make predictions using our data! 

**Spending 100k (high) with band attractiveness of 3 (low):**
```{r, include = T, eval = T, echo = T}

#intercept + (advertising_slope * X1) + (attractiveness_slope * X2)

26.34 + (0.092351 * 100) + (16.27 * 2)
```

**Spending 10k (low) with band attractiveness of 10 (high):**
```{r, include = T, eval = T, echo = T}

#intercept + (advertising_slope * X1) + (attractiveness_slope * X2)

26.34 + (0.092351 * 10) + (16.27 * 10)
```

**Spending 100k (high) with band attractiveness of 10 (high):**
```{r, include = T, eval = T, echo = T}

#intercept + (advertising_slope * X1) + (attractiveness_slope * X2)

26.34 + (0.092351 * 100) + (16.27 * 10)
```


## Next Week: Interactions

The two independent variables (predictors) can also work together on the dependent variable. In that case, the effects are called **interaction effects**. We will cover this next week!  


