---
title: "Nonparametric Statistics"
date: "Week 8"
output:
  rmdformats::readthedown:
    highlight: kate
css: custom.css
---

<style type="text/css">

body{ /* Normal  */
      font-size: 26px;
      line-height: 1.2;
  }
p{ /* Normal  */
      font-size: 26px;
      line-height: 1.2;
  }
 li{ /* Normal  */
      font-size: 24px;
      line-height: 1.2;
       padding: 5px 0px;
  }
td {  /* Table  */
  font-size: 8px;
}
h1.title {
  font-size: 38px;

}
h1 { /* Header 1 */
  font-size: 48px;

}
h2 { /* Header 2 */
    font-size: 36px;

}
h3 { /* Header 3 */
  font-size: 30px;


}

h4 { /* Header 4 */
  font-size: 28px;


}

code.r{ /* Code block */
    font-size: 40px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 18px;
}
</style>

```{r setup, echo=FALSE, cache=FALSE, warning = F, message = F}
library(knitr)
library(rmdformats)

## Global options
options(max.print="40")
opts_chunk$set(echo=FALSE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```


```{r, echo = F,cache=FALSE, warning = F, message = F}
library(knitr)
library(formatR)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)

```

## Aims

When and why do we use non-parametric tests?

* Mann-Whitney-Wilcoxon rank-sum test
* Mann-Whitney-Wilcoxon signed-rank test
* Chi Squared Test 

Ranking data  

Interpretation of results  

Reporting results  

## When to Use Nonparametric Tests

Non-parametric tests are used when assumptions of parametric tests are not met  

It is not always possible to correct for problems with the distribution of a data set 

* In these cases we have to use non-parametric tests
* They make fewer assumptions about the type of data on which they can be used

## The Mann-Whitney-Wilcoxon rank-sum test

The non-parametric equivalent of the independent t-test  

Use to test differences between two conditions in which different participants have been used

### Ranking Data

The test works on the principle of ranking the data for each group: 

* Lowest score = a rank of 1 
* Next highest score = a rank of 2, and so on 
* Tied ranks are given the same rank: the average of the potential ranks

For an unequal group size

* The test statistic *(Ws) = sum of ranks in the group that contains the least people*

For an equal group size

* *Ws  = the value of the smaller summed rank*

Add up the ranks for the two groups and take the lowest of these sums to be our test statistic  

The analysis is carried out on the ranks rather than the actual data.

### An Example

A neurologist investigated the depressant effects of certain recreational drugs

* Tested 20 clubbers
* 10 were given an ecstasy tablet to take on a Saturday night
* 10 were allowed to drink only alcohol 
* Levels of depression were measured using the Beck Depression Inventory (BDI) the day after and midweek

Rank the data ignoring the group to which a person belonged

* A similar number of high and low ranks in each group suggests depression levels do not differ between the groups
* A greater number of high ranks in the ecstasy group than the alcohol group suggests the ecstasy group is more depressed than the alcohol group

### Running the test in R

```{r, eval = F, echo = T}
wilcox.test(outcome ~ predictor, data = dataFrame, paired = FALSE/TRUE)
```

To compute a basic Mann-Whitney-Wilcoxon test for our Sunday data we could execute:
```{r, eval = F, echo = T}
sunModel<-wilcox.test(sundayBDI ~ drug, data = drugData)
sunModel
```

For the Wednesday data:
```{r, eval = F, echo = T}
wedModel<-wilcox.test(wedsBDI ~ drug, data = drugData)
wedModel
```

#### Output from the Wilcoxon Rank-Sum Test 

![](images/week8/image1.png)

### Reporting the Results

Depression levels in ecstasy users (Mdn = 17.50) did not differ significantly from alcohol users (Mdn = 16.00) the day after the drugs were taken, W = 35.5, p = .286. However, by Wednesday, ecstasy users (Mdn = 33.50) were significantly more depressed than alcohol users (Mdn = 7.50), W = 4, p < .001.

## Comparing Two Related Conditions: the Mann-Whitney-Wilcoxon Signed-Rank Test 

Uses:

* To compare two sets of scores, when these scores come from the same participants

Imagine the experimenter in the previous example was interested in the change in depression levels for each of the two drugs

* We still have to use a non-parametric test because the distributions of scores for both drugs were non-normal on one of the two days

We want to run our analysis on the alcohol and ecstasy groups separately; therefore, our first job is to split the dataframe into two using the subset() function:

```{r, eval = F, echo = T}
alcoholData<-subset(drugData, drug == "Alcohol")
ecstasyData<-subset(drugData, drug == "Ecstacy")
```

To run the analysis for the alcohol group execute:

```{r, eval = F, echo = T}
alcoholModel<-wilcox.test(alcoholData$wedsBDI, alcoholData$sundayBDI, paired = TRUE, correct= FALSE)
alcoholModel
```

and for the ecstasy group:

```{r, eval = F, echo = T}
ecstasyModel<-wilcox.test(ecstasyData$wedsBDI, ecstasyData$sundayBDI, paired = TRUE, correct= FALSE)
ecstasyModel
```

#### Output

![](images/week8/image2.png)

#### Reporting the results 

For ecstasy users, depression levels were significantly higher on Wednesday (Mdn = 33.50) than on Sunday (Mdn = 17.50), p = .047. However, for alcohol users the opposite was true: depression levels were significantly lower on Wednesday (Mdn = 7.50) than on Sunday (Mdn = 16.0), p = .012.


## Chi-Squared Test

The **Chi Square statistic** is commonly used for testing relationships between categorical variables. The null hypothesis of the Chi-Square test is that no relationship exists on the categorical variables in the population; they are independent.  

**How does the Chi-Square statistic work?**  

The Chi-Square statistic is most commonly used to evaluate Tests of Independence when using a crosstabulation (also known as a bivariate table).  

Crosstabulation presents the distributions of two categorical variables simultaneously, with the intersections of the categories of the variables appearing in the cells of the table.  

The Test of Independence assesses whether an association exists between the two variables by comparing the observed pattern of responses in the cells to the pattern that would be expected if the variables were truly independent of each other.  

Calculating the Chi-Square statistic and comparing it against a critical value from the Chi-Square distribution allows the researcher to assess whether the observed cell counts are significantly different from the expected cell counts.

<a href = "https://www.statisticssolutions.com/using-chi-square-statistic-in-research/"> definition adapted from here </a>

## Chi-Squared Example

The data is a contingency table containing 13 housetasks and their distribution in the couple:

* rows are the different tasks
* values are the frequencies of the tasks done :
  + by the wife only
  + alternatively
  + by the husband only
  + or jointly
  
![](images/week8/image3.png)

### Graphical display of contengency tables

```{r, eval = F, echo = T}
library("gplots")
# 1. convert the data as a table
dt <- as.table(as.matrix(housetasks))
# 2. Graph
balloonplot(t(dt), main ="housetasks", xlab ="", ylab="",
            label = FALSE, show.margins = FALSE)
```

### Chi-square test basics
Chi-square test examines whether rows and columns of a contingency table are statistically significantly associated  

**Null hypothesis (H0):** the row and the column variables of the contingency table are independent  

**Alternative hypothesis (H1):** row and column variables are dependent  

For each cell of the table, we have to calculate the expected value under null hypothesis  

## Compute chi-square test in R
Chi-square statistic can be easily computed using the function *chisq.test()* as follows:

```{r, eval = F, echo = T}
chisq <- chisq.test(housetasks)
chisq
```

![](images/week8/image4.png)

In our example, the row and the column variables are statistically significantly associated (p-value < 0.05)  

### Nature of the dependence between the row and the column variables

As mentioned above the total Chi-square statistic is 1944.456196  

If you want to know the most contributing cells to the total Chi-square score, you just have to calculate the Chi-square statistic for each cell  

Pearson residuals can be easily extracted from the output of the function chisq.test():  

```{r, eval = F, echo = T}
round(chisq$residuals, 3)
```

![](images/week8/image5.png)

#### Let’s visualize Pearson residuals using the package corrplot:

```{r, eval = F, echo = T}
library(corrplot)
corrplot(chisq$residuals, is.cor = FALSE)
```


![](images/week8/image6.png)

For a given cell, the size of the circle is proportional to the amount of the cell contribution  

The sign of the standardized residuals is also very important to interpret the association between rows and columns as explained below  

Positive residuals are in blue. Positive values in cells specify an attraction (positive association) between the corresponding row and column variables  

* In the image above, it’s evident that there are an association between the column Wife and the rows Laundry, Main_meal  
* There is a strong positive association between the column Husband and the row Repair  

Negative residuals are in red. This implies a repulsion (negative association) between the corresponding row and column variables  

* For example the column Wife are negatively associated (~ “not associated”) with the row Repairs
* There is a repulsion between the column Husband and, the rows Laundry and Main_meal

<a href = "http://www.sthda.com/english/wiki/chi-square-test-of-independence-in-r"> example adapted from here </a>
